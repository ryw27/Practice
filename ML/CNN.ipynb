{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c822ae78-bdcb-4489-904a-bb13683abdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import numpy as np\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import tqdm as tq\n",
    "from tinygrad.nn.datasets import mnist\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "300f898e-5761-430c-9101-e49fc5c26e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train, X_test, Y_test = mnist()\n",
    "X_train = X_train.numpy().astype(np.float32)\n",
    "Y_train = Y_train.numpy().astype(np.int8)\n",
    "X_test = X_test.numpy().astype(np.float32)\n",
    "Y_test = Y_test.numpy().astype(np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b21dd8b5-44d7-42cc-8fe4-a681aa41d415",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1,6,4)\n",
    "        self.conv2 = nn.Conv2d(6,16,4)\n",
    "        self.linear = nn.Linear(256, 10) #bs is 256\n",
    "    def forward(self,x):\n",
    "        c1 = F.relu(self.conv1(x)) #relu activation function after first convolution layer\n",
    "        l2 = F.max_pool2d(c1, (2,2))#subsampling, breaking down to process better\n",
    "        c2 = F.relu(self.conv2(l2)) #second convolution\n",
    "        l4 = F.max_pool2d(c2,(2,2))\n",
    "        l4 = torch.flatten(l4,1) #flatten down to one dimension to run through fully connected layer/mlp\n",
    "        out = self.linear(l4)\n",
    "        return out\n",
    "model = ConvNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77aa2e69-b052-4df8-94b9-bd2dd1d7ef72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run \n",
    "\n",
    "optim = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "batch_size = 256\n",
    "for i in range(100):\n",
    "    samp = np.random.randint(0, X_train.shape[0], size=(batch_size))\n",
    "    X = torch.tensor(X_train[samp])\n",
    "    Y = torch.tensor(Y_train[samp]).long()\n",
    "\n",
    "    optim.zero_grad() #reset gradients\n",
    "    out = model(X) #forward pass\n",
    "    cat = torch.argmax(out, dim=1) \n",
    "    #print(\"cat\",cat.shape)\n",
    "    accuracy = (cat == Y).float().mean() #check how many are right\n",
    "    #run backprop\n",
    "    loss = loss_func(out, Y)\n",
    "\n",
    "    loss.backward()\n",
    "    #do grad descent \n",
    "    \n",
    "    optim.step()\n",
    "    loss, accuracy = loss.item(), accuracy.item()\n",
    "    accuracies.append(accuracy)\n",
    "    losses.append(loss)\n",
    "    if k % 100 == 0:\n",
    "        print(\"loss %.2f accuracy %.2f\" % (loss, accuracy))\n",
    "plt.ylim(-1,2)\n",
    "plt.plot(losses)\n",
    "plt.plot(accuracies)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
